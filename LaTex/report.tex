\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{caption}

\setlength{\tabcolsep}{15pt}
\renewcommand{\arraystretch}{1.7}

\begin{document}
\title{Road Segmentation of Satellite Images Using \\ Convolutional Neural Networks}

\author{
	Jean Gillain\\
	\textit{sciper no. 331411}
	\and
	 David Desboeufs\\
	 \textit{sciper no. 287441}
	 \and 
	 Mathieu Caboche\\
	 \textit{sciper no. 282182}
}

\maketitle

\begin{abstract}
	Outlining roads on satellite images can be useful for creating accurate maps of all four corners of the earth. Machine Learning enables us to automate this task. In fact, convolutional neural networks (CNNs) are a suitable tool for image recognition and thus road segmentation as well. In this project we design a CNN and evaluate the resulting model. We compare our various CNN designs to a baseline model obtained from a basic CNN.
	
\end{abstract}

\section{Introduction}

% TODO : Sum up the process of data extension and CNN design. 


\section{The Google Maps Satellite Images Data Set}
\label{sec:dataset}
The data set we are given to train a model on consists of $100$ satellite images from Google Maps. The pictures look like they were all taken from a very similar locations, probably one single city. As such, the given data set is not very diverse. Furthermore, it is quite limited in size. This explains why we need to enlarge our data set. Note that our training set is a fraction of the same Google Maps satellite images. Thus, we expect that our road classifier can be trained to perform well for segmenting roads of that particular city or similar cities. Its performance when applied to radically different roads and areas can however not be certified.

In section \ref{ssec:data_proc} we describe how the data set was enlarged and how we managed to train our CNN on a substantial amount of images, so as to yield proper results.


\section{Models and Methods}
\label{sec:methods}


\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{correlation.png}
		\captionsetup{aboveskip=0.4cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Correlation matrix of all features}
		\label{fig:correlation}
	\end{minipage}
	\hspace{0.05cm}
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{grid_search.png}
		\captionsetup{aboveskip=0.1cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Accuracies obtained for different polynomial extension degrees and $\lambda$ values}
		\label{fig:grid_search}
	\end{minipage}
\end{figure}

\subsection{Creating a Large Data Set} 
\label{ssec:data_proc}

We used a variety of methods and combined them in order to have a lot of data available to train our CNN. 

Firstly, although we only have 100 images at our disposal for training, we actually train on smaller images that are subsets of the given images. For example, we can choose to train on images of size $20 \times 20$ pixels. This means that we get $400$ training input sets for one single image (the given images have size $400 \times 400$). We are effectively partitioning large images into many small images, which has the added benefit of reducing the parameter count of our CNN.

Another common way to enlarge ou data set we used is to apply various transformations on the original images. By rotating, cropping and flipping the large scale images, we can obtain many variations of that image which prove very useful for training. Note that in this process we might lose image resolution, for example if we rotate and then crop. However that loss of resolution is effectively a zoom, another transformation. Any additional transformed images can only increase the sturdiness of our trained model. 
% TODO : which library for transformations?

We explored further ways of increasing the amount of input data we can feed to our CNN. One way to attain this is to give as input the edges found in an image along with the pixels of that image. We simply process the image with a Canny edge detector. This yields a gray-scale image indicating where edges are to be found. We do not extract vectors from the resulting gray-scale picture, but only give the latter's pixels as an additional input to our CNN.
% TODO : which library for canny?


\subsection{Evolution of our CNN} 
\label{ssec:technique}

\begin{equation} 
	\label{eq1}
	f(x) = \log{\frac{1}{1+x}}
\end{equation}


\subsection{Model Validation} 
\label{ssec:model_validation}


\section{Results}
\label{sec:results}

\begin{table}[h]
	\begin{tabular}{ |c|c| } 
		\hline
		\textbf{Regression Technique} & \textbf{Accuracy (Avg.) - STD}  \\
		\hline
		Least Squares GD (and SGD) & $68.5\%$ - $0.20\%$ \\ 
		Least Squares & $74.4\%$ - $0.25\%$ \\ 
		Ridge Regression & $74.4\%$ - $0.24\%$ \\ 
		Logistic Regression & $65.7\%$ - $0.29\%$ \\ 
		Newton Logistic Regression & $65.7\%$ - $0.29\%$ \\ 
		Our Best Regression & $81.3\%$ - $0.82\%$ \\ 
		\hline
	\end{tabular}
	\captionsetup{aboveskip=0.3cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
	\caption{Accuracy of Various Models Obtained by Our Regressions}
	\label{tab:model_accuracy}
\end{table}

\section{Conclusion}
\label{sec:conclusion}


\end{document}
