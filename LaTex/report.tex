\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{caption}

\setlength{\tabcolsep}{15pt}
\renewcommand{\arraystretch}{1.7}

\begin{document}
\title{Road Segmentation of Satellite Images Using \\ Convolutional Neural Networks}

\author{
	Jean Gillain\\
	\textit{sciper no. 331411}
	\and
	 David Desboeufs\\
	 \textit{sciper no. 287441}
	 \and 
	 Mathieu Caboche\\
	 \textit{sciper no. 282182}
}

\maketitle

\begin{abstract}
	Outlining roads on satellite images can be useful for creating accurate maps of all four corners of the earth. Machine Learning enables us to automate this task. In fact, convolutional neural networks (CNNs) are a suitable tool for image recognition, and thus, for road segmentation as well. In this project we design a CNN and evaluate the resulting model. We compare our various CNN designs to a baseline model obtained from a basic CNN.
	
\end{abstract}

\section{Introduction}

% TODO : Sum up the process of data extension and CNN design. 


\section{The Google Maps Satellite Images Data Set}
\label{sec:dataset}
The data set we are given to train a model on, consists of $100$ satellite images from Google Maps. The pictures look like they were all taken from very similar locations, probably one single city. As such, the given data set is not very diverse. Furthermore, it is quite limited in size. This explains why we need to enlarge our data set. Note that our training set is a fraction of the same Google Maps satellite images. Thus, we expect that our road classifier can be trained to perform well for segmenting roads of that particular city/area or similar cities/areas. Its performance when applied to radically different roads and areas can however not be certified.

In section \ref{ssec:data_proc} we describe how the data set is enlarged and how we manage to train our CNN on a substantial amount of images, so as to yield proper results.


\section{Models and Methods}
\label{sec:methods}


\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{correlation.png}
		\captionsetup{aboveskip=0.4cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Correlation matrix of all features}
		\label{fig:correlation}
	\end{minipage}
	\hspace{0.05cm}
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{grid_search.png}
		\captionsetup{aboveskip=0.1cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Accuracies obtained for different polynomial extension degrees and $\lambda$ values}
		\label{fig:grid_search}
	\end{minipage}
\end{figure}

\subsection{Creating a Large Data Set} 
\label{ssec:data_proc}

We experimented with various methods in order to enhance our given data set. Let us examine these methods and sum up which of them proved useful, and which did not.

\textit{[PATCHES]}   
Firstly, although we only have $100$ images at our disposal for training, we actually train on smaller images that are subsets of the given images, we call them patches. For example, we can choose to train on images of size $20 \times 20$ pixels. This means that we get $400$ training input sets for one single image (the given images have size $400 \times 400$). We are effectively partitioning large images into many small images, which has the added benefit of reducing the parameter count of our CNN.

\textit{[TRANSFORMATIONS]}   
Another common way to enlarge a data set, which we use, is to apply various transformations on the original images. By rotating, cropping and flipping a large scale image, we can obtain many variations of that image which prove very useful for training. Note that in this process we might lose image resolution, for example if we rotate and then crop. However that loss of resolution is effectively a zoom, another transformation. Any additional transformed images can only increase the sturdiness of our trained model. 

More specifically, we rotate the training images by multiples of $45$ degrees. When necessary, (for example when doing $45$ degree rotations) we crop the images so as to avoid the resulting black areas. In cases where cropping is necessary, we lose close to one third of the image's resolution and content. To counterbalance this, we upscale those images. That way, all resulting images share the exact same size. This process leaves us with $800$ images after processing the original $100$. To further increase the size of our training data set, we mirror every image over its vertical axis. Eventually, we find ourselves with a total of $1600$ images to train our model on.

\textit{[EDGES]}   
We explored further ways of increasing the amount of input data we can feed to our CNN. One way to attain this, is to give as input the edges found in an image, along with the pixels of that image. We process the gray-scale converted image with a Sobel filter using the Scipy library. This yields a gray-scale image indicating where edges are to be found. We do not extract vectors from the resulting gray-scale picture, but only give the latter's pixels as an additional input to our CNN.

\textit{[DISTANCE]}   
One additional information we extract and feed to our CNN, is what we like to call the \textit{distance} of an image. This  \textit{distance} is the norm of the vector we obtain as a result of subtracting from every pixel's RGB value a value we call the \textit{mean road RGB color}. We compute the \textit{mean road RGB color} by averaging all RGB colors that are known to represent a piece of road, i.e. all pixels of our training set that we know to be part of a road (by looking at the ground-truth images). The \textit{distance} of an image can thus be represented as another image, of identical dimensions. It is an image that represents, at every pixel, how closely the color of the latter resembles the known color of a road. The resulting image can be fed to our CNN as additional data.

\textit{[STANDARDIZATION]}   
Finally, we also perform standardization of our data. 
%TODO: add standardization


However, both methods \textit{EDGES} and \textit{DISTANCE} did not prove useful. In fact, it is not necessary to manually extract features such as those when we train a model using a CNN. A good neural net will automatically learn those features. The method \textit{PATCHES} is not just useful, it is necessary. In fact, to limit parameter count, it is good to work on smaller inputs (and outputs). We will detail in later sections the patch sizes used for various designs. \textit{TRANSFORMATIONS} proved extremely useful as well.
%TODO: standardization?



\subsection{Evolution of Our Neural Network} 
\label{ssec:technique}

In the following we will discuss several CNN designs we experimented with, both successful and unsuccessful. For will name every design / method, so that we can reference them later on.

Before we dive into the detailed neural net designs we developed, let us list a few general ideas we experimented with.

\textit{[SEQUENTIAL CNNs]}  
 One first such idea is to use a combination of two or more convolutional neural networks. In our case, we tried using two CNNs one after the other. The second neural net takes as input the predictions made by the model resulting from the first CNN. In this design, the second net mainly acts as some kind of image filter that tries to fix the mistakes made by the first one. It will fill missing gaps in road predictions and remove some unwanted noise.

\textit{[PARALLEL CNNs]}  
Another idea would be to use two or more convolutional neural networks in parallel, and, for example, merge their results using another (following-up) CNN. In this case, we would need to apply those parallel convolutional neural nets on different inputs. However, as mentioned in section \ref{ssec:data_proc}, using inputs other than the images' RGB values is unnecessary, or at least not very useful in most cases. Thus, we did not develop this idea further.

We will now detail more precise design decisions, the ideas behind those designs, and their effect on model performance.

\textit{[PATCH SIZE]} 
As mentioned in section \ref{ssec:data_proc}, we divide all of our images (both training and test images) into patches of smaller size. The choice of patch size depends on the implemented neural net design. If we want to use a large number of batches, we need to reduce the patch size. In our case, most implementations use a patch size of $16 \times 16$ pixels. This coincides with the evaluation criteria. Some of our implementations classify an image pixel by pixel, some classify patch by patch. In general, using patches of side $16$, we found that our model's predictions were better evaluated when predicting patch by patch.

\textit{[PATCH EXTENSIONS]} 
Let us consider an implementation using patches of side $16$ pixels, and classifying patch by patch. In this case, we notice that patches containing on object that overlays a road are often ill-classified. To counteract this issue, we would like the classifier to take into account the pixels that surround the patch it is working on. That is, when classifying a patch of side 16, we give the classifier a larger input, i.e. a patch of side $24$. Using this method we can improve our prediction accuracies, as shown in figure \ref{fig:extensions}.

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{correlation.png}
		\captionsetup{aboveskip=0.4cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Accuracies of a simple CNN with patches of side $16$px}
		\label{fig:noExtensions}
	\end{minipage}
	\hspace{0.05cm}
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{grid_search.png}
		\captionsetup{aboveskip=0.1cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Accuracies of a the same CNN with patch extensions ($24$px)}
		\label{fig:extensions}
	\end{minipage}
\end{figure}
% TODO: comparison -> graph

\textit{[ALEX NET]}
%TODO: our variant of alex net and its results

\textit{[U-NET]}
%TODO: our u-net variant and its results



\subsection{Post-Processing} 
\label{ssec:post}

After observing that many of the predictions made by our designed models contained a fair amount of noise, we decided to use some post-processing to get clearer results. The observed noise mostly consists of single patches wrongly classified as road, lying in between patches correctly classified as background. We also notice some patches surrounded by road-patches that are classified as background.

To subvert this issue, we apply a convolution filter to the resulting prediction. The convolution looks at blocks of patches of size $3 \times 3$, and determines if the central patch should be marked as road or not, depending on the $8$ patches surrounding it. The effect of this filter is shown in figure \ref{fig:post}. On average, this post-processing method improved our prediction accuracy by $XXX\%$.
%TODO: ^^^ fill

\begin{figure}[h]
	\centering
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{correlation.png}
		\captionsetup{aboveskip=0.4cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Prediction without post-processing}
		\label{fig:noPost}
	\end{minipage}
	\hspace{0.05cm}
	\begin{minipage}[b]{0.48\linewidth}
		\includegraphics[width=\textwidth]{grid_search.png}
		\captionsetup{aboveskip=0.1cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
		\caption{Prediction with post-processing}
		\label{fig:post}
	\end{minipage}
\end{figure}
% TODO: figure comparison -> original, post processed

\subsection{Model Validation} 
\label{ssec:model_validation}

To validate our model we first experimented with k-fold cross validation. However, in order to get satisfactory results from the latter method, we would need to train our model multiple times for every implementation we designed. Since for most implementations a single training round can take a few hours, comparing our models using k-fold cross validation is not an option.

% TODO: how do we validate? Division in train test and evaluation sets

\section{Final Design}
\label{sec:final}

\section{Results}
\label{sec:results}

\begin{table}[h]
	\begin{tabular}{ |c|c| } 
		\hline
		\textbf{Regression Technique} & \textbf{Accuracy (Avg.) - STD}  \\
		\hline
		Least Squares GD (and SGD) & $68.5\%$ - $0.20\%$ \\ 
		Least Squares & $74.4\%$ - $0.25\%$ \\ 
		Ridge Regression & $74.4\%$ - $0.24\%$ \\ 
		Logistic Regression & $65.7\%$ - $0.29\%$ \\ 
		Newton Logistic Regression & $65.7\%$ - $0.29\%$ \\ 
		Our Best Regression & $81.3\%$ - $0.82\%$ \\ 
		\hline
	\end{tabular}
	\captionsetup{aboveskip=0.3cm,justification=centering, margin=0.1cm, labelfont=footnotesize, textfont=footnotesize}
	\caption{Accuracy of Various Models Obtained by Our Regressions}
	\label{tab:model_accuracy}
\end{table}

\section{Conclusion}
\label{sec:conclusion}


\end{document}
